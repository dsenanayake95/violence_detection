{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa4b621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib.util\n",
    "from tensorflow.lite.python.interpreter import Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e284d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'coco_mobilenet'\n",
    "MODEL_NAME = 'detect.tflite'\n",
    "LABELMAP_NAME = 'labelmap.txt'\n",
    "min_conf_threshold = float(0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82d3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get path to current working directory\n",
    "CWD_PATH = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Path to .tflite file, which contains the model that is used for object detection\n",
    "PATH_TO_CKPT = os.path.join(CWD_PATH, MODEL_DIR, MODEL_NAME)\n",
    "\n",
    "# Path to label map file\n",
    "PATH_TO_LABELS = os.path.join(CWD_PATH, MODEL_DIR, LABELMAP_NAME)\n",
    "\n",
    "# Load the label map\n",
    "with open(PATH_TO_LABELS, 'r') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "# First label is '???', which has to be removed.\n",
    "if labels[0] == '???':\n",
    "    del (labels[0])\n",
    "    \n",
    "# Load the Tensorflow Lite model.\n",
    "interpreter = Interpreter(model_path=PATH_TO_CKPT)\n",
    "\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d91efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "height = input_details[0]['shape'][1]\n",
    "width = input_details[0]['shape'][2]\n",
    "\n",
    "floating_model = (input_details[0]['dtype'] == np.float32)\n",
    "\n",
    "input_mean = 127.5\n",
    "input_std = 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aa530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to video dataset\n",
    "startpath_nonviolent ='../raw_data/videos_dataset/Real Life Violence Dataset/Violence/'\n",
    "\n",
    "# Create a list with all the file names\n",
    "vids_list = os.listdir(startpath_nonviolent)\n",
    "\n",
    "# Establish a pattern to detect only videos\n",
    "pattern = \"*.mp4\"\n",
    "\n",
    "# Create a list of names of every file in the dataset\n",
    "filename_list = []\n",
    "for file in vids_list:  \n",
    "    if fnmatch.fnmatch(file, pattern):\n",
    "        filename_list.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize video \n",
    "for video in filename_list[0:1000]:\n",
    "    num = np.random.randint(0, 1_000_000_000)\n",
    "    \n",
    "    capture = cv2.VideoCapture(f'../raw_data/videos_dataset/Real Life Violence Dataset/Violence/{video}')\n",
    "\n",
    "    # Create window\n",
    "    cv2.namedWindow('Object detector', cv2.WINDOW_NORMAL)\n",
    "\n",
    "    # Get the dimensions of the video used for rectangle creation\n",
    "    imW  = capture.get(3)  # float `width`\n",
    "    imH = capture.get(4)  # float `height`\n",
    "\n",
    "    while capture.isOpened(): \n",
    "        # Capture the video frame\n",
    "        ret, frame = capture.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Start timer (for calculating frame rate)\n",
    "        t1 = cv2.getTickCount()\n",
    "\n",
    "        # Acquire frame and resize to expected shape [1xHxWx3]\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_resized = cv2.resize(frame_rgb, (width, height))\n",
    "        input_data = np.expand_dims(frame_resized, axis=0)\n",
    "\n",
    "        # Normalize pixel values if using a floating model (i.e. if model is non-quantized)\n",
    "        if floating_model:\n",
    "            input_data = (np.float32(input_data) - input_mean) / input_std\n",
    "\n",
    "        # Perform the actual detection by running the model with the image as input\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Retrieve detection results\n",
    "        # Bounding box coordinates of detected objects\n",
    "        boxes = interpreter.get_tensor(output_details[0]['index'])[0]  \n",
    "\n",
    "        # Class index of detected objects\n",
    "        classes = interpreter.get_tensor(output_details[1]['index'])[0]  \n",
    "\n",
    "        # Confidence of detected objects\n",
    "        scores = interpreter.get_tensor(output_details[2]['index'])[0]  \n",
    "\n",
    "        # Locate indexes for persons classes only\n",
    "        if 0 in classes:\n",
    "            idx_list = [idx for idx, val in enumerate(classes) if val ==0]\n",
    "\n",
    "            # Reassign bounding boxes only to detected people\n",
    "            boxes = [boxes[i] for i in idx_list]\n",
    "\n",
    "            # Loop over all detections and draw detection box if confidence is above minimum threshold\n",
    "            # Look at the entire frame therefore comment out this part # for box in boxes:\n",
    "            for i in range(len(scores)):\n",
    "                if ((scores[i] > min_conf_threshold) and (scores[i] <= 1.0)):\n",
    "\n",
    "                    # Get bounding box coordinates and draw box for all people detected\n",
    "                    if len(boxes) > 0:\n",
    "                        # Find the top-most top\n",
    "                        top = min([i[0] for i in boxes])\n",
    "                        # Find the left-most left\n",
    "                        left = min([i[1] for i in boxes])\n",
    "                        # Find the bottom-most bottom\n",
    "                        bottom = max([i[2] for i in boxes])\n",
    "                        # Find the right-most right\n",
    "                        right = max([i[3] for i in boxes])\n",
    "\n",
    "                        # Convert bounding lines into coordinates\n",
    "                        # Interpreter can return coordinates that are outside of image dimensions, \n",
    "                        # Need to force them to be within image using max() and min()\n",
    "                        ymin = int(max(1, (top * imH)))\n",
    "                        xmin = int(max(1, (left * imW)))\n",
    "                        ymax = int(min(imH, (bottom * imH)))\n",
    "                        xmax = int(min(imW, (right * imW)))\n",
    "\n",
    "                        # Save every 10th frame\n",
    "                        if num%10 == 0:\n",
    "                            # Save frame into a new folder\n",
    "                            cv2.imwrite(f\"../raw_data/cropped_dataset/{num}.jpg\", frame[ymin:ymax, xmin:xmax])\n",
    "                        \n",
    "                        num += 1\n",
    "                        \n",
    "                        # Build a rectangle\n",
    "                        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n",
    "\n",
    "\n",
    "        # All the results have been drawn on the frame, so it's time to display it.\n",
    "        # cv2.imshow('Object detector', frame)\n",
    "\n",
    "        # Press 'q' to quit\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            capture.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "# cleanup\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "violence_detection",
   "language": "python",
   "name": "violence_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
