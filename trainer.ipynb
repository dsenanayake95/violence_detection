{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "trainer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1XMzlgec1fMgz-_3ClyV_u8p8cohQjxTE",
      "authorship_tag": "ABX9TyNKyq4REGYgO6G+QyHrTFRm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsenanayake95/violence_detection/blob/master/trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ3dFTLnL55S"
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import os, sys\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YlkEN-YMyJc"
      },
      "source": [
        "# tensorflow imports\n",
        "from tensorflow.keras import Sequential, layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from tensorflow.keras.layers import LSTM, Dense, Flatten\n",
        "from tensorflow.keras.utils import to_categorical, image_dataset_from_directory\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import save_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jc9DZsKKL96W",
        "outputId": "a20f75a7-e90a-4432-de14-68fe7407dc45"
      },
      "source": [
        "!git clone https://github.com/dsenanayake95/violence_detection.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'violence_detection' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H3mUcJFMmnc",
        "outputId": "b5d99fd7-32be-477d-e68e-7fc1855fc382"
      },
      "source": [
        "%cd violence_detection/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/violence_detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUz39p0bMpqV"
      },
      "source": [
        "CWD_PATH = sys.path.append(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMTzozO_NRhb",
        "outputId": "2b565d7e-bcb1-4d80-b369-6580c2564079"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyFWoyaGUkh7"
      },
      "source": [
        "def preprocess(image, label):\n",
        "    image = image / 255\n",
        "    return image, label\n",
        "\n",
        "\n",
        "def augment(image, label):\n",
        "    if np.random.rand(1) < 0.2:\n",
        "        image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.1, upper=0.4)\n",
        "    return image, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd_7yegqKqPg"
      },
      "source": [
        "# ZIP = \"raw_data/violence.zip\"  #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< CHANGE\n",
        "# CWD_PATH = os.getcwd()\n",
        "ROOT = \"drive/MyDrive/violence_detection/raw_data/cropped_dataset\"\n",
        "# BUCKET_NAME = \"YOURBUCKETNAME\"  #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< CHANGE\n",
        "\n",
        "# TARGET_IMSIZE = (224,224)\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Build a trainer class\n",
        "class Trainer():\n",
        "    def __init__(self, root):\n",
        "        self.train_generator = None\n",
        "        self.test_generator = None\n",
        "        self.val_generator = None\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.eval_ = None\n",
        "        self.root = ROOT # data root path\n",
        "\n",
        "    # def unzipper(self):\n",
        "    #     with zipfile.ZipFile(ZIP, 'r') as zip_ref:\n",
        "    #         zip_ref.extractall(\".\")\n",
        "    #     return self\n",
        "\n",
        "    # Splitting frames into train, test\n",
        "    def split(self):\n",
        "        self.train_ratio = 0.8\n",
        "        self.test_ratio = 0.2\n",
        "\n",
        "        classes_dir = ['violence', 'non_violence'] # all labels\n",
        "\n",
        "        for label in classes_dir:\n",
        "            os.makedirs(ROOT + '/train/' + label)\n",
        "            os.makedirs(ROOT + '/test/' + label)\n",
        "\n",
        "            # Creating partitions of the data after shuffling\n",
        "            src = ROOT + \"/\" + label  # Folder to copy images from\n",
        "\n",
        "            self.allFileNames = os.listdir(src)\n",
        "            np.random.shuffle(self.allFileNames)\n",
        "            self.train_FileNames, self.test_FileNames = np.split(np.array(self.allFileNames),\n",
        "                                                                  [int(len(self.allFileNames)* (1 - (self.test_ratio)))])\n",
        "                                                                  \n",
        "\n",
        "            self.train_FileNames = [src+'/'+ name for name in self.train_FileNames.tolist()]\n",
        "            self.test_FileNames = [src+'/' + name for name in self.test_FileNames.tolist()]\n",
        "\n",
        "            # Copy-pasting images\n",
        "            for name in self.train_FileNames:\n",
        "                shutil.copy(name, ROOT + '/train/' + label)\n",
        "\n",
        "            for name in self.test_FileNames:\n",
        "                shutil.copy(name, ROOT + '/test/' + label)\n",
        "\n",
        "\n",
        "    # Generate data + Augment frames in the train set\n",
        "    def generate_data(self):\n",
        "        train_dir = ROOT + \"/train/\"\n",
        "\n",
        "        self.train_generator = image_dataset_from_directory(train_dir,\n",
        "                                                    labels='inferred',\n",
        "                                                    seed=123,\n",
        "                                                    label_mode='binary',\n",
        "                                                    batch_size=16,\n",
        "                                                    image_size=(224,224),\n",
        "                                                    shuffle=True,\n",
        "                                                    subset='training',\n",
        "                                                    validation_split=0.33)\n",
        "        \n",
        "        self.val_generator = image_dataset_from_directory(train_dir,\n",
        "                                                    labels='inferred',\n",
        "                                                    seed=123,\n",
        "                                                    label_mode='binary',\n",
        "                                                    batch_size=16,\n",
        "                                                    image_size=(224,224),\n",
        "                                                    shuffle=True,\n",
        "                                                    subset='validation',\n",
        "                                                    validation_split=0.33)\n",
        "\n",
        "        test_dir = ROOT + \"/test/\"\n",
        "\n",
        "        self.test_generator = image_dataset_from_directory(test_dir,\n",
        "                                                    labels='inferred',\n",
        "                                                    seed=123,\n",
        "                                                    label_mode='binary',\n",
        "                                                    batch_size=16,\n",
        "                                                    image_size=(224,224),\n",
        "                                                    shuffle=True)\n",
        "        \n",
        "        return self.train_generator, self.val_generator, self.test_generator\n",
        "\n",
        "    # Transfer learning model\n",
        "    def load_model(self, transfer=VGG19, dense_n=512, lr= 0.000134):\n",
        "        transfer_model = transfer(include_top=False, \n",
        "                                  weights=\"imagenet\",\n",
        "                                  input_shape=(224, 224, 3), \n",
        "                                  pooling=\"max\", \n",
        "                                  classes=2,\n",
        "                                  )\n",
        "        \n",
        "        transfer_model.trainable = False\n",
        "        \n",
        "        self.model = tf.keras.models.Sequential([\n",
        "                                transfer_model,\n",
        "                                Flatten(),\n",
        "                                Dense(dense_n, activation='relu'),\n",
        "                                Dense(dense_n, activation='relu'),\n",
        "                                Dense(dense_n/2, activation='relu'),\n",
        "                                Dense(dense_n/4, activation='relu'),\n",
        "                                Dense(dense_n/8, activation='relu'),\n",
        "                                Dense(dense_n/16, activation='relu'),\n",
        "                                Dense(dense_n/32, activation='relu'),\n",
        "                                Dense(1, activation='sigmoid')\n",
        "                                    ])\n",
        "\n",
        "        opt = optimizers.Adam(learning_rate=lr)\n",
        "        self.model.compile(loss='binary_crossentropy',\n",
        "                      optimizer=opt,\n",
        "                      metrics=['accuracy'])\n",
        "        \n",
        "        return self.model\n",
        "\n",
        "    # Instantiate  + fit model\n",
        "    def run(self, epochs=5):\n",
        "\n",
        "        train = self.train_generator\n",
        "        val = self.val_generator\n",
        "        test = self.test_generator\n",
        "\n",
        "        train_prep = train.map(augment, num_parallel_calls=AUTOTUNE).map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "        val_prep = val.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "        test_prep = test.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "        train_final = train_prep.cache()\n",
        "        train_final = train_prep.prefetch(AUTOTUNE)\n",
        "\n",
        "        val_final = val_prep.prefetch(AUTOTUNE) # check if cache could be useful\n",
        "        test_final = test_prep.prefetch(AUTOTUNE)\n",
        "\n",
        "        model = self.load_model()\n",
        "\n",
        "        es = EarlyStopping(monitor='val_accuracy',\n",
        "                           mode='max',\n",
        "                           patience=20,\n",
        "                           verbose=1,\n",
        "                           restore_best_weights=True)\n",
        "\n",
        "\n",
        "        model.fit(train_final,\n",
        "                  validation_data=val_final,\n",
        "                  epochs=epochs,\n",
        "                  verbose=1,\n",
        "                  callbacks=es)\n",
        "        \n",
        "        model.evaluate(test_final)\n",
        "\n",
        "        self.model = model\n",
        "\n",
        "        return self\n",
        "\n",
        "    # save the model\n",
        "    def save(self, name):\n",
        "        save_model(self.model, f'drive/MyDrive/violence_detection/models/{name}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ0OWXHLysKw",
        "outputId": "3b920bc2-8c13-4ced-f57a-7900357f1ba2"
      },
      "source": [
        "print(\"loading trainer...\")\n",
        "trainer = Trainer(root=ROOT)\n",
        "print(\"trainer loaded\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading trainer...\n",
            "trainer loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnXy9ZEtyoFq"
      },
      "source": [
        "# print(\"spliting the data into train, test...\")\n",
        "# trainer.split()\n",
        "# print(\"data successfully split\")\n",
        "# print('Total images: ', len(trainer.allFileNames))\n",
        "# print('Training: ', len(trainer.train_FileNames))\n",
        "# print('Testing: ', len(trainer.test_FileNames))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jjm2toQjNmXr",
        "outputId": "0f710e62-3dfb-4c30-c681-bf1899294bec"
      },
      "source": [
        "print(\"augmenting data now...\")\n",
        "trainer.generate_data()\n",
        "print(\"data successfully prepped\")\n",
        "print(\"loading models...\")\n",
        "\n",
        "models = [MobileNet, ResNet50, VGG19]\n",
        "epochs = [10, 20, 50]\n",
        "lrs = [0.000134, 0.0013, 0.0112]\n",
        "\n",
        "optimal_model = VGG19\n",
        "optimal_lr = 0.0002\n",
        "epoch = 50\n",
        "\n",
        "print(f\"using learning rate of {optimal_lr} and {optimal_model}\")\n",
        "trainer.load_model(transfer=optimal_model, dense_n=512, lr=optimal_lr)\n",
        "print(f\"{optimal_model} successfully loaded\")\n",
        "print(\"running model...\")\n",
        "trainer.run(epochs=epoch)\n",
        "print(\"model completed\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "augmenting data now...\n",
            "Found 9047 files belonging to 2 classes.\n",
            "Using 6062 files for training.\n",
            "Found 9047 files belonging to 2 classes.\n",
            "Using 2985 files for validation.\n",
            "Found 2262 files belonging to 2 classes.\n",
            "data successfully prepped\n",
            "loading models...\n",
            "using learning rate of 0.0002 and <function VGG19 at 0x7f096d8ab830>\n",
            "<function VGG19 at 0x7f096d8ab830> successfully loaded\n",
            "running model...\n",
            "Epoch 1/50\n",
            "379/379 [==============================] - 110s 267ms/step - loss: 0.6877 - accuracy: 0.5501 - val_loss: 0.6761 - val_accuracy: 0.5836\n",
            "Epoch 2/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.6641 - accuracy: 0.5996 - val_loss: 0.6102 - val_accuracy: 0.6740\n",
            "Epoch 3/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.6383 - accuracy: 0.6333 - val_loss: 0.7118 - val_accuracy: 0.6452\n",
            "Epoch 4/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.6157 - accuracy: 0.6554 - val_loss: 0.6575 - val_accuracy: 0.6680\n",
            "Epoch 5/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.5876 - accuracy: 0.6823 - val_loss: 0.7882 - val_accuracy: 0.6456\n",
            "Epoch 6/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5760 - accuracy: 0.6894 - val_loss: 0.7083 - val_accuracy: 0.6851\n",
            "Epoch 7/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5666 - accuracy: 0.6968 - val_loss: 0.6653 - val_accuracy: 0.7112\n",
            "Epoch 8/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5472 - accuracy: 0.7201 - val_loss: 0.6938 - val_accuracy: 0.6948\n",
            "Epoch 9/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5454 - accuracy: 0.7168 - val_loss: 0.6791 - val_accuracy: 0.7039\n",
            "Epoch 10/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5274 - accuracy: 0.7285 - val_loss: 0.6773 - val_accuracy: 0.7025\n",
            "Epoch 11/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.5384 - accuracy: 0.7215 - val_loss: 0.6667 - val_accuracy: 0.7129\n",
            "Epoch 12/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5317 - accuracy: 0.7300 - val_loss: 0.7724 - val_accuracy: 0.7112\n",
            "Epoch 13/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5354 - accuracy: 0.7267 - val_loss: 0.7151 - val_accuracy: 0.7062\n",
            "Epoch 14/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5271 - accuracy: 0.7265 - val_loss: 0.8971 - val_accuracy: 0.6660\n",
            "Epoch 15/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5026 - accuracy: 0.7450 - val_loss: 0.8382 - val_accuracy: 0.6821\n",
            "Epoch 16/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.5103 - accuracy: 0.7418 - val_loss: 0.7123 - val_accuracy: 0.6995\n",
            "Epoch 17/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4961 - accuracy: 0.7526 - val_loss: 0.8711 - val_accuracy: 0.7045\n",
            "Epoch 18/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4967 - accuracy: 0.7453 - val_loss: 0.8738 - val_accuracy: 0.6972\n",
            "Epoch 19/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4862 - accuracy: 0.7611 - val_loss: 0.7588 - val_accuracy: 0.7149\n",
            "Epoch 20/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4892 - accuracy: 0.7552 - val_loss: 0.8444 - val_accuracy: 0.7059\n",
            "Epoch 21/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4762 - accuracy: 0.7598 - val_loss: 1.0208 - val_accuracy: 0.6834\n",
            "Epoch 22/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4891 - accuracy: 0.7540 - val_loss: 0.9809 - val_accuracy: 0.6774\n",
            "Epoch 23/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4680 - accuracy: 0.7699 - val_loss: 0.8303 - val_accuracy: 0.6978\n",
            "Epoch 24/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4758 - accuracy: 0.7623 - val_loss: 0.7533 - val_accuracy: 0.7015\n",
            "Epoch 25/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4638 - accuracy: 0.7725 - val_loss: 0.8272 - val_accuracy: 0.6948\n",
            "Epoch 26/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4644 - accuracy: 0.7727 - val_loss: 1.1860 - val_accuracy: 0.6874\n",
            "Epoch 27/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4775 - accuracy: 0.7613 - val_loss: 0.8670 - val_accuracy: 0.6948\n",
            "Epoch 28/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4453 - accuracy: 0.7902 - val_loss: 1.2233 - val_accuracy: 0.6938\n",
            "Epoch 29/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4619 - accuracy: 0.7742 - val_loss: 1.0844 - val_accuracy: 0.6945\n",
            "Epoch 30/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4546 - accuracy: 0.7757 - val_loss: 1.0502 - val_accuracy: 0.6878\n",
            "Epoch 31/50\n",
            "379/379 [==============================] - 93s 243ms/step - loss: 0.4485 - accuracy: 0.7735 - val_loss: 1.1387 - val_accuracy: 0.6992\n",
            "Epoch 32/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4527 - accuracy: 0.7745 - val_loss: 1.3563 - val_accuracy: 0.6901\n",
            "Epoch 33/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4544 - accuracy: 0.7760 - val_loss: 0.8806 - val_accuracy: 0.7042\n",
            "Epoch 34/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4310 - accuracy: 0.7917 - val_loss: 1.0154 - val_accuracy: 0.7203\n",
            "Epoch 35/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4419 - accuracy: 0.7824 - val_loss: 1.0146 - val_accuracy: 0.7102\n",
            "Epoch 36/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4351 - accuracy: 0.7884 - val_loss: 0.9834 - val_accuracy: 0.7363\n",
            "Epoch 37/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4175 - accuracy: 0.8006 - val_loss: 1.3122 - val_accuracy: 0.7082\n",
            "Epoch 38/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4255 - accuracy: 0.7987 - val_loss: 1.3139 - val_accuracy: 0.7112\n",
            "Epoch 39/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4272 - accuracy: 0.7958 - val_loss: 1.6198 - val_accuracy: 0.6898\n",
            "Epoch 40/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4152 - accuracy: 0.7969 - val_loss: 1.5619 - val_accuracy: 0.6968\n",
            "Epoch 41/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4208 - accuracy: 0.8006 - val_loss: 2.2926 - val_accuracy: 0.6503\n",
            "Epoch 42/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4143 - accuracy: 0.8004 - val_loss: 1.6371 - val_accuracy: 0.6801\n",
            "Epoch 43/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4246 - accuracy: 0.7971 - val_loss: 1.3630 - val_accuracy: 0.6972\n",
            "Epoch 44/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4270 - accuracy: 0.7925 - val_loss: 2.0630 - val_accuracy: 0.6405\n",
            "Epoch 45/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4058 - accuracy: 0.8086 - val_loss: 1.3941 - val_accuracy: 0.6941\n",
            "Epoch 46/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.4102 - accuracy: 0.8047 - val_loss: 1.2775 - val_accuracy: 0.6881\n",
            "Epoch 47/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.3920 - accuracy: 0.8136 - val_loss: 1.3377 - val_accuracy: 0.6958\n",
            "Epoch 48/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4116 - accuracy: 0.7974 - val_loss: 1.2067 - val_accuracy: 0.7156\n",
            "Epoch 49/50\n",
            "379/379 [==============================] - 93s 244ms/step - loss: 0.4109 - accuracy: 0.8034 - val_loss: 1.4594 - val_accuracy: 0.6965\n",
            "Epoch 50/50\n",
            "379/379 [==============================] - 93s 245ms/step - loss: 0.3955 - accuracy: 0.8078 - val_loss: 1.5728 - val_accuracy: 0.7015\n",
            "142/142 [==============================] - 26s 182ms/step - loss: 1.6629 - accuracy: 0.7082\n",
            "model completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff2rIYosxexS",
        "outputId": "3b57e2bf-18be-4699-d776-8cb1e485cde3"
      },
      "source": [
        "trainer.save('VGG19_lr_0.0002_model_v3')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: drive/MyDrive/violence_detection/models/VGG19_lr_0.0002_model_v3/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QneA1sbG7buZ"
      },
      "source": [
        "# testing learning rate of 0.000134\n",
        "# Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
        "# 17227776/17225924 [==============================] - 0s 0us/step\n",
        "# 17235968/17225924 [==============================] - 0s 0us/step\n",
        "# <function MobileNet at 0x7fa2c9483f80> successfully loaded\n",
        "# running model...\n",
        "# Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# 80142336/80134624 [==============================] - 1s 0us/step\n",
        "# 80150528/80134624 [==============================] - 1s 0us/step\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 1193s 3s/step - loss: 0.6551 - accuracy: 0.5858 - val_loss: 0.6938 - val_accuracy: 0.6459\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 85s 227ms/step - loss: 0.5587 - accuracy: 0.6993 - val_loss: 0.6648 - val_accuracy: 0.6866\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5437 - accuracy: 0.7145 - val_loss: 0.6574 - val_accuracy: 0.7068\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.4986 - accuracy: 0.7392 - val_loss: 1.0704 - val_accuracy: 0.6579\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.4922 - accuracy: 0.7502 - val_loss: 0.6337 - val_accuracy: 0.7079\n",
        "# 139/139 [==============================] - 290s 2s/step - loss: 0.6657 - accuracy: 0.7073\n",
        "# model completed\n",
        "# testing learning rate of 0.0013\n",
        "# <function MobileNet at 0x7fa2c9483f80> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 86s 228ms/step - loss: 0.6590 - accuracy: 0.5937 - val_loss: 0.7864 - val_accuracy: 0.6010\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 85s 229ms/step - loss: 0.5893 - accuracy: 0.6773 - val_loss: 0.8023 - val_accuracy: 0.6637\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 85s 227ms/step - loss: 0.5560 - accuracy: 0.7035 - val_loss: 0.6319 - val_accuracy: 0.7106\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5160 - accuracy: 0.7409 - val_loss: 0.6737 - val_accuracy: 0.7103\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.4972 - accuracy: 0.7499 - val_loss: 0.6114 - val_accuracy: 0.7373\n",
        "# 139/139 [==============================] - 22s 152ms/step - loss: 0.6356 - accuracy: 0.7290\n",
        "# model completed\n",
        "# testing learning rate of 0.0112\n",
        "# <function MobileNet at 0x7fa2c9483f80> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 87s 229ms/step - loss: 0.6518 - accuracy: 0.6067 - val_loss: 0.7612 - val_accuracy: 0.6161\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5639 - accuracy: 0.6952 - val_loss: 0.8945 - val_accuracy: 0.6651\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5299 - accuracy: 0.7226 - val_loss: 0.9526 - val_accuracy: 0.6527\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5151 - accuracy: 0.7325 - val_loss: 0.6300 - val_accuracy: 0.6914\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 85s 227ms/step - loss: 0.4996 - accuracy: 0.7423 - val_loss: 0.8421 - val_accuracy: 0.6774\n",
        "# 139/139 [==============================] - 22s 152ms/step - loss: 0.8496 - accuracy: 0.6852\n",
        "# model completed\n",
        "# testing learning rate of 0.000134\n",
        "# Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "# 94773248/94765736 [==============================] - 1s 0us/step\n",
        "# 94781440/94765736 [==============================] - 1s 0us/step\n",
        "# <function ResNet50 at 0x7fa2c94233b0> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 86s 229ms/step - loss: 0.6285 - accuracy: 0.6176 - val_loss: 0.6695 - val_accuracy: 0.6664\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5531 - accuracy: 0.7053 - val_loss: 0.8439 - val_accuracy: 0.6647\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5339 - accuracy: 0.7205 - val_loss: 0.6758 - val_accuracy: 0.7086\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 85s 227ms/step - loss: 0.5014 - accuracy: 0.7443 - val_loss: 0.8785 - val_accuracy: 0.6842\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.4881 - accuracy: 0.7482 - val_loss: 0.8376 - val_accuracy: 0.6955\n",
        "# 139/139 [==============================] - 22s 153ms/step - loss: 0.8548 - accuracy: 0.6920\n",
        "# model completed\n",
        "# testing learning rate of 0.0013\n",
        "# <function ResNet50 at 0x7fa2c94233b0> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 87s 229ms/step - loss: 0.6474 - accuracy: 0.6117 - val_loss: 0.7140 - val_accuracy: 0.6534\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5739 - accuracy: 0.6880 - val_loss: 0.6225 - val_accuracy: 0.7140\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5362 - accuracy: 0.7160 - val_loss: 1.1103 - val_accuracy: 0.5925\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5183 - accuracy: 0.7333 - val_loss: 0.6688 - val_accuracy: 0.7048\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.4946 - accuracy: 0.7532 - val_loss: 0.7494 - val_accuracy: 0.7041\n",
        "# 139/139 [==============================] - 22s 152ms/step - loss: 0.7481 - accuracy: 0.7096\n",
        "# model completed\n",
        "# testing learning rate of 0.0112\n",
        "# <function ResNet50 at 0x7fa2c94233b0> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 87s 229ms/step - loss: 0.6353 - accuracy: 0.6154 - val_loss: 0.6828 - val_accuracy: 0.6236\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5626 - accuracy: 0.7035 - val_loss: 0.8722 - val_accuracy: 0.6373\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5252 - accuracy: 0.7274 - val_loss: 0.7368 - val_accuracy: 0.6949\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5172 - accuracy: 0.7340 - val_loss: 0.7165 - val_accuracy: 0.6962\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.4931 - accuracy: 0.7514 - val_loss: 0.6676 - val_accuracy: 0.6983\n",
        "# 139/139 [==============================] - 22s 152ms/step - loss: 0.6766 - accuracy: 0.6915\n",
        "# model completed\n",
        "# testing learning rate of 0.000134\n",
        "# <function VGG19 at 0x7fa2c942a830> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 87s 229ms/step - loss: 0.6412 - accuracy: 0.6072 - val_loss: 1.0407 - val_accuracy: 0.5818\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5618 - accuracy: 0.6902 - val_loss: 0.5933 - val_accuracy: 0.6904\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5220 - accuracy: 0.7268 - val_loss: 1.0402 - val_accuracy: 0.6616\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5116 - accuracy: 0.7419 - val_loss: 0.8672 - val_accuracy: 0.6514\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.4948 - accuracy: 0.7565 - val_loss: 0.6020 - val_accuracy: 0.7390\n",
        "# 139/139 [==============================] - 22s 153ms/step - loss: 0.6151 - accuracy: 0.7258\n",
        "# model completed\n",
        "# testing learning rate of 0.0013\n",
        "# <function VGG19 at 0x7fa2c942a830> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 87s 229ms/step - loss: 0.6341 - accuracy: 0.6262 - val_loss: 0.6483 - val_accuracy: 0.6870\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5606 - accuracy: 0.6969 - val_loss: 0.6535 - val_accuracy: 0.6613\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5435 - accuracy: 0.7138 - val_loss: 0.6393 - val_accuracy: 0.7202\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5091 - accuracy: 0.7419 - val_loss: 0.7926 - val_accuracy: 0.6918\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 85s 229ms/step - loss: 0.5014 - accuracy: 0.7489 - val_loss: 0.5873 - val_accuracy: 0.7175\n",
        "# 139/139 [==============================] - 22s 153ms/step - loss: 0.6038 - accuracy: 0.7069\n",
        "# model completed\n",
        "# testing learning rate of 0.0112\n",
        "# <function VGG19 at 0x7fa2c942a830> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 87s 230ms/step - loss: 0.6340 - accuracy: 0.6153 - val_loss: 0.7726 - val_accuracy: 0.6445\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 85s 229ms/step - loss: 0.5534 - accuracy: 0.7082 - val_loss: 0.6633 - val_accuracy: 0.6853\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5185 - accuracy: 0.7333 - val_loss: 0.6877 - val_accuracy: 0.7110\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.5031 - accuracy: 0.7495 - val_loss: 0.6473 - val_accuracy: 0.6925\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 85s 228ms/step - loss: 0.4970 - accuracy: 0.7497 - val_loss: 0.7174 - val_accuracy: 0.7164\n",
        "# 139/139 [==============================] - 22s 153ms/step - loss: 0.7383 - accuracy: 0.7096"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEQy0EoZQ-Ur"
      },
      "source": [
        "# Results of different learning rates [0.000134, 0.0013, 0.0112]\n",
        "\n",
        "# testing learning rate of 0.000134\n",
        "# <function MobileNet at 0x7f043ca1f0e0> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 91s 241ms/step - loss: 0.4837 - accuracy: 0.7607 - val_loss: 0.4096 - val_accuracy: 0.8123\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 90s 241ms/step - loss: 0.3201 - accuracy: 0.8642 - val_loss: 0.3310 - val_accuracy: 0.8493\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 90s 241ms/step - loss: 0.2593 - accuracy: 0.8966 - val_loss: 0.2573 - val_accuracy: 0.8932\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 89s 240ms/step - loss: 0.2088 - accuracy: 0.9165 - val_loss: 0.2640 - val_accuracy: 0.8990\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 89s 240ms/step - loss: 0.1601 - accuracy: 0.9368 - val_loss: 0.2874 - val_accuracy: 0.8976\n",
        "# 139/139 [==============================] - 23s 162ms/step - loss: 0.2827 - accuracy: 0.8916\n",
        "# model completed\n",
        "# testing learning rate of 0.0013\n",
        "# <function MobileNet at 0x7f043ca1f0e0> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 92s 242ms/step - loss: 0.4560 - accuracy: 0.7910 - val_loss: 0.3296 - val_accuracy: 0.8596\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 90s 241ms/step - loss: 0.3060 - accuracy: 0.8722 - val_loss: 0.3121 - val_accuracy: 0.8736\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 90s 241ms/step - loss: 0.2587 - accuracy: 0.8921 - val_loss: 0.2595 - val_accuracy: 0.8921\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 90s 240ms/step - loss: 0.2211 - accuracy: 0.9123 - val_loss: 0.2750 - val_accuracy: 0.8911\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 90s 240ms/step - loss: 0.1768 - accuracy: 0.9300 - val_loss: 0.3069 - val_accuracy: 0.8805\n",
        "# 139/139 [==============================] - 23s 161ms/step - loss: 0.2776 - accuracy: 0.8803\n",
        "# model completed\n",
        "# testing learning rate of 0.0112\n",
        "# <function MobileNet at 0x7f043ca1f0e0> successfully loaded\n",
        "# running model...\n",
        "# Epoch 1/5\n",
        "# 371/371 [==============================] - 91s 241ms/step - loss: 0.4707 - accuracy: 0.7792 - val_loss: 0.4468 - val_accuracy: 0.7925\n",
        "# Epoch 2/5\n",
        "# 371/371 [==============================] - 90s 241ms/step - loss: 0.3214 - accuracy: 0.8578 - val_loss: 0.3648 - val_accuracy: 0.8411\n",
        "# Epoch 3/5\n",
        "# 371/371 [==============================] - 90s 241ms/step - loss: 0.2648 - accuracy: 0.8883 - val_loss: 0.2665 - val_accuracy: 0.8887\n",
        "# Epoch 4/5\n",
        "# 371/371 [==============================] - 89s 240ms/step - loss: 0.2112 - accuracy: 0.9133 - val_loss: 0.2556 - val_accuracy: 0.9038\n",
        "# Epoch 5/5\n",
        "# 371/371 [==============================] - 90s 241ms/step - loss: 0.1758 - accuracy: 0.9249 - val_loss: 0.2367 - val_accuracy: 0.9103\n",
        "# 139/139 [==============================] - 23s 160ms/step - loss: 0.2171 - accuracy: 0.9115"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgqVZgX7ZJhQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}